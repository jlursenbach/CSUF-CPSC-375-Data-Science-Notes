# Text Analysis

[text analysis.pptx](Text%20Analysis%20a487788136574906b905288d22a08f33/text_analysis.pptx)

History:

became important because of Social media analytics trends

- Metrics: demise of ‚Äúvanity‚Äù metrics (likes, dislikes)
- Decline of corporations and government trust, the rise of micro-influencers
- Social commerce
- Social listening: tracking conversations, identify recurring phrases, words or brands, identify uncovered demands and opportunities

# Terminology

- **Corpus:**  the dataset
- **Document:**  the unit of data for classification
    - News article
    - Tweet
    - Amazon review
- **Sentence:**  unit of written language
- **Token:** a word in a document
- A **corpus** contains documents; each document is a collection of tokens
- **Vocabulary:** distinct words in a dataset

# Key Concepts:

![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled.png)

### Visualization Techniques:

- word Frequency
- Word Clouds
- etc

### Data Preperation:

- A token: a meaningful unit of text
a word, a sentence
- Tokenization: splitting the text into these smaller linguistic units

### What is a word?

e.g., are cat and cats the same word?
September and Sept?
zero and oh?
Is _ a word?  * ? ) . ,
How many words are there in don‚Äôt ?  Gonna ?
Japanese and Chinese text: how do we identify a word?
Word is well defined unit in western languages ‚Äì e.g. Chinese has different notion of semantic unit

### Data preparation:

Split the text into smaller linguistic units
words are separated by whitespace, punctuation marks or line breaks
Not perfect: ‚ÄúLos Angeles‚Äù and ‚Äúrock 'n' roll‚Äù?
Which token to use?
‚ÄúN-grams‚Äù: consecutive sequences of words
‚Äúthis is‚Äù (2-gram or bigram)

![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%201.png)

# Text Processing in R:

## Libraries:

Two major approaches/libraries

- library(tidytext)
    - - Uses the tidy data principle
    - - tibble data type
- Uses data wrangling functions for text processing
    - - library(tm) # text mining
    - - Vcorpus  data type to hold text data
    - - Special functions to transform the data

## TidyText:

### Convert tweets to the tidy text format

- Install ‚Äútidytext‚Äù package
- unnest_tokens()
    
    ```r
    data %>% unnest_tokens(input = "text", output = "word")
    ```
    
- Breaks up strings in the input column to tokens
- Tokens are in the new output column

```r
mydata %>% unnest_tokens(input = "text", output = "word")
```

![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%202.png)

[20220627 Classwork 1](Text%20Analysis%20a487788136574906b905288d22a08f33/20220627%20Classwork%201%20fe40ab0c50a840ec8b589e7dc7a4f3ed.md)

Common pre-processing steps
Convert to lower case
Remove punctuation
Must be careful when processing social media data
#hashtags, @mentions, URLs
Not punctuation, are case-sensitive
unnest_tokens() understands tweets

```r
mydata %>% unnest_tokens(input=‚Ä¶, output=‚Ä¶, token="tweets")
```

### Word frequency

(basis for text classification)

Count how many times each word appears
A common type of analysis
‚ÄúCounts‚Äù also called ‚Äúfrequency‚Äù
In R:

```r
‚Ä¶ %>% group_by(word) %>% summarise(n())
Shorter, more readable:
‚Ä¶ %>% count(word)
```

[20220627 Classwork 2](Text%20Analysis%20a487788136574906b905288d22a08f33/20220627%20Classwork%202%20888a76132e2f46239906da0ec11316df.md)

### Plotting results:

To create a bar chart:

ggplot(data=airquality) +geom_col(aes(Month,Ozone))

add: + coord_flip()  if you want to flip it 90degrees

### Word Clouds:

library(wordcloud)
Install the wordcloud package first

wordcloud(mywords, mywordcounts, max.words =20)

[20220627 Classwork 3](Text%20Analysis%20a487788136574906b905288d22a08f33/20220627%20Classwork%203%2058ffa2385a234b128955e1286b266883.md)

### Stopwords:

```r
> stop_words
# A tibble: 1,149 x 2
   word        lexicon
   <chr>       <chr>  
 1 a           SMART  
 2 a's         SMART  
 3 able        SMART  
 4 about       SMART  
 5 above       SMART  
 6 according   SMART  
 7 accordingly SMART  
 8 across      SMART  
 9 actually    SMART  
10 after       SMART
```

to remove stopwords from your dataset use: 

```r
antijoin()
```

if you try ‚Äúantijoin‚Äù you need to name the columns that are being anti-joined to the same name:

stop_words words are in a column names ‚Äúword‚Äù 

so either change the column name, 
or use the antijoin (by=c(token=word)) to indicate which columns should be compared. 

```r

```

![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%203.png)

### Stemming

- Different forms of the same word (e.g., learns, learned, learning,‚Ä¶) have similar meaning but different spelling
    - problematic for text data analysis
- stemming
    - process of transforming a word into its most important part: its stem
- Stemming merges different forms of a word into one

NOTE: 

<aside>
üí° Stemming is done using a sequence of rules
NOT by looking up a dictionary
Therefore:
It is computationally efficient
But not perfect

</aside>

![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%204.png)

![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%205.png)

### Wordstemming in R

wordStem()

- Part of the SnowballC package
    - Automatically installed with tidytext
    - Just load it:
        - library(SnowballC)

```r
‚Ä¶ %>%  mutate(word = wordStem(word))
```

[20220627 Classwork 4](Text%20Analysis%20a487788136574906b905288d22a08f33/20220627%20Classwork%204%20ad2958ff94ab4eec82d4de71d04f52fe.md)

who decides words:

in english: no one

in French: the government

# Document Categorization

### Given:

set of documents labeled with content categories

### The goal:

to build a model which would automatically assign right content categories to new unlabeled documents.

Content categories can be:

unstructured (e.g., Reuters) or

structured (e.g., Yahoo, Medline)

## Document categorization

![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%206.png)

Amazon is trying to detect fake reviews. 

- assume that you have a bunch of reviews that may be fake or not‚Ä¶..
- can we predict whether a new review is real or fake?
    - What is it the most similar to?

nearest neighbor classification is one way you‚Äôd do this

## Text representation for classification

### So how do we comput the nearest neighbor?

Represent documents as vectors
Each term/word is a dimension
Terms are axes of the space
Documents are points or vectors in this space
Very high-dimensional:
tens of thousands of dimensions for English text
But very sparse vectors
Many words in the corpus, but each document contains only a few words
most entries are zero

## Bag of words model:

Represents documents as vectors:

Completely ignore the linguistic structure within the text
Order of words is not important
John is quicker than Mary and Mary is quicker than John have the same vectors
Called ‚ÄúBag-Of-Words‚Äù or ‚ÄúVector-Space‚Äù model
After converting to a vector-space model, can do:
Classification
Clustering

![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%207.png)

Each document is represented by a binary vector ‚àà {0,1}|V|

### Term-document count matrices:

Consider the number of occurrences of a term in a document:
Each document is a count vector

includes word count:

![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%208.png)

### Term Frequency:

It‚Äôs even better to provide a frequency related to the document size

The term frequency tft,d of term t in document d is defined as the number of times that t occurs in d
Typically, term frequency is normalized:
Divide by the number of words in the document
ùëõ_(ùë°,ùëë): number of times word t appears in document d

![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%209.png)

### Document frequency

So we also want to look at how rare words are in GENREAL. It‚Äôll show if a rare word is used more often in a document.  If a word like: ‚ÄúThe‚Äù occurres everywhere, so it shouldn‚Äôt really ahve any weight 

Words that appear in only a few documents are more informative than frequent terms
Frequent words: the, is
Rare word: arachnocentric
Want a high weight for rare terms like arachnocentric.

### Inverse document frequency (idf)

As a word becomes more frequent in general use (or in a set of documents) the inverse document frequency becomes smaller. 

dft is the document frequency of t:
number of documents that contain t
Idf (Inverse document frequency) of t:
ùêºùëëùëì_ùë°=log‚Å°„Äñ(ùëÅ/ùëëùëì_ùë°)„Äó
N: Total number of documents
We use log (N/dft) instead of N/dft to ‚Äúdampen‚Äù the effect of idf

![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%2010.png)

So now we have the 

## tf-idf weighting

so we than multiply the Term Frequency multiplied by the inverse document frequency:

The tf-idf weight of a term is the product of its tf weight and its idf weight
ùë§_(ùë°,ùëë)=tf_(ùë°,ùëë)√ólog‚Å°„Äñ(ùëÅ/df_ùë°)„Äó

This is the most commonly used weighting scheme
Increases with
the number of occurrences within a document
the rarity of the term in the collection

we stopped on Slide 47: 

[20220627](Text%20Analysis%20a487788136574906b905288d22a08f33/20220627%20bfe133ab9a104af1976c74de92aadebf.md)

### Tf_idf in tidytext

- tbl %>% bind_tf_idf(term, document, n)
    - term: column containing the words
    - document : column identifying the document
    - n: column containing the word count
- Important:
    - Table should contain only one row per term-document pair
    - Do a count(document, term) before bind_tf_idf()

![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%2011.png)

- note/remember (these are equivalent
    
    ![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%2012.png)
    
    ### We shoudln‚Äôt use euclidean distance for this kind of similarity comparrison:
    
    The Euclidean distance between q
    and d2 is large even though the
    distribution of terms in the document q and the distribution of
    terms in the document d2 are
    very similar.
    
    ![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%2013.png)
    
    ### We use angle instead of distance
    
    Thought experiment: take a document A and append it to itself. Call this document B.
    ‚ÄúSemantically‚Äù A and B have the same content
    The Euclidean distance between the two documents can be quite large
    But, the angle between the two documents is 0, corresponding to maximal similarity
    
    ### From angles to cosines
    
    The following two notions are equivalent.
    Rank documents in decreasing order of the angle between query and document
    Rank documents in increasing order  of cosine(query,document)
    Cosine is a monotonically decreasing function for the interval [0o, 180o]
    
    ### Length normalization
    
    A vector can be normalized by dividing each of its components by its length:
    
    ![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%2014.png)
    
    Dividing a vector by its length makes it a unit vector
    Effect on the two documents A and B: they have identical vectors after length-normalization.
    Long and short documents now have comparable weights
    
    ### Cosine for length-normalized vectors
    
    For length-normalized vectors, cosine similarity is simply the dot product:
    
    ![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%2015.png)
    
    ### Cosine similarity illustrated
    
    ![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%2016.png)
    
    ### Example of simple cosine similarity:
    
    ![Untitled](Text%20Analysis%20a487788136574906b905288d22a08f33/Untitled%2017.png)
    
    you can see there that v1 and v3 have nothing in common so their cosine similarity is 0 
    While 1 and 2 did have some similarities so their cosine similarity is close to 1
    
    (our hw is to code this for a much larger dataset)